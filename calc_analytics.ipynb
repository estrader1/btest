{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b4f2f1-2832-4cf8-be45-39c5003f8abf",
   "metadata": {},
   "source": [
    "### read data and calc returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a40331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo \n",
    "# 1. add dividends\n",
    "# 2. add fundamental factors \n",
    "# 3. run and save daily and weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6926e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_raw_ingest(filepath):\n",
    "    # read in raw data\n",
    "    df = pd.read_csv(filepath, parse_dates = True).set_index(['ID','DATE'])\n",
    "    \n",
    "    # clean up duplicates\n",
    "    df = df[~df.index.duplicated()].reset_index() # causes multiindex issues later\n",
    "    \n",
    "    # drop rows with missing close prices\n",
    "    df = df.dropna(subset = ['close'])\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    # convert date to datetime\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # sort by ID and DATE\n",
    "    df = df.sort_values(['ID', 'DATE'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233caa5",
   "metadata": {},
   "source": [
    "#### read univ and idx data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48178774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_raw_ingest('data_raw_univ_rtyohlc_2014.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33624986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SPX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxdf = data_raw_ingest('data_raw_idx.csv')\n",
    "spxdf = idxdf.query('ID == \"SPX Index\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "spxdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ee7cb-0972-48f5-a1d3-a251a7d371c6",
   "metadata": {},
   "source": [
    "### merge univ and idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = df.merge(spxdf[['DATE','close']].rename(columns ={'close':'idx_close'}), on = 'DATE', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1737cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92394532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958037fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the resample to work properly need this\n",
    "mdf = mdf.set_index(['ID','DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920779d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_raw_resample(df):\n",
    "    sum_columns = ['volume']  # Specify columns to sum (can be multiple, e.g., ['close', 'high'])\n",
    "\n",
    "    # Get all columns except 'ID' and 'DATE' (assuming these shouldn't be aggregated)\n",
    "    all_columns = [col for col in df.columns if col not in ['ID', 'DATE']]\n",
    "\n",
    "    # Create aggregation dictionary: default 'last' for all, override with 'sum' for specified columns\n",
    "    agg_dict = {col: 'last' for col in all_columns}\n",
    "    agg_dict.update({col: 'sum' for col in sum_columns})\n",
    "\n",
    "    def resample_group(id_group):\n",
    "        id_val, group = id_group\n",
    "        resampled = group.resample('M', level='DATE').agg(agg_dict)\n",
    "        resampled['ID'] = id_val  # Add back the ID\n",
    "        return resampled\n",
    "\n",
    "    # Parallel processing function\n",
    "    def parallel_resample(df):\n",
    "        # Split the dataframe by ID\n",
    "        groups = list(df.groupby(level='ID'))\n",
    "        \n",
    "        # Process groups in parallel\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(resample_group)(group) for group in groups\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        final_df = pd.concat(results)\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    # Perform the parallel resampling\n",
    "    result = parallel_resample(df)\n",
    "\n",
    "    return result.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f30d4f-a5e1-4e9a-8ef8-498edd68be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = data_raw_resample(mdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9409d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a72fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01dfed4a-6c12-4b5a-bd76-a7108179e09f",
   "metadata": {},
   "source": [
    "### functions - rolling reg / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829952-4a5b-4de6-87bb-943fe9f14e7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def calculate_simple_returns(df):\n",
    "    \"\"\"Helper function to calculate returns and excess returns\"\"\"\n",
    "    df = df.copy()\n",
    "    df['stock_return'] = df.groupby('ID')['close'].pct_change()\n",
    "    df['idx_return'] = df.groupby('ID')['idx_close'].pct_change()\n",
    "    df['stock_excess_return'] = df['stock_return'] - df['idx_return']\n",
    "\n",
    "    return df\n",
    "\n",
    "def rolling_residual_variance(df, window_size, dependent_var, independent_vars):\n",
    "    \"\"\"\n",
    "    Performs rolling regression in parallel using joblib.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame containing the data.\n",
    "        window_size: Size of the rolling window.\n",
    "        dependent_var: Name of the dependent variable column.\n",
    "        independent_vars: List of names of independent variable columns.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with the regression coefficients for each window.\n",
    "        Returns None if there are issues.\n",
    "    \"\"\"\n",
    "\n",
    "    n_rows = len(df)\n",
    "    results = []\n",
    "\n",
    "    def _regress_window(i):\n",
    "        if i < window_size -1:\n",
    "            return None # Handle edge cases at beginning of dataframe\n",
    "        window_data = df.iloc[i - window_size + 1:i + 1]\n",
    "\n",
    "        X = window_data[independent_vars].values\n",
    "        y = window_data[dependent_var].values.reshape(-1,1) # Reshape y for sklearn\n",
    "\n",
    "        if len(window_data) < window_size or np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "            return None  # Handle cases where the window is incomplete or contains NaNs.\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        # Step 3: Calculate the residuals\n",
    "        residuals = y - y_pred\n",
    "\n",
    "        # Step 4: Compute the residual variance\n",
    "        residual_variance = np.var(residuals, ddof=1) \n",
    "\n",
    "        return {'index': df.index[i], 'residual_variance': residual_variance} # Include index for proper merging\n",
    "\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(_regress_window)(i) for i in range(n_rows))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Filter out None results (from edge cases or NaN windows)\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "\n",
    "    if not valid_results: # Check if all results are invalid\n",
    "        return None\n",
    "\n",
    "    results_df = pd.DataFrame(valid_results)#.set_index('index')\n",
    "    \n",
    "    # Handle multiindex\n",
    "    if isinstance(results_df['index'].iloc[0], tuple):\n",
    "        results_df[list(df.index.names)] = results_df['index'].apply(pd.Series)\n",
    "        results_df = results_df.drop(columns='index').set_index(list(df.index.names))\n",
    "    else:\n",
    "        results_df = results_df.set_index('index')\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "def rolling_regression(df, window_size, dependent_var, independent_vars, reg_type='OLS', alpha=1.0):\n",
    "    \"\"\"\n",
    "    Performs rolling regression in parallel using joblib, with options for OLS, Ridge, and Lasso.\n",
    "    Returns np.nan for coefficients when X or y contains NaN values.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame containing the data.\n",
    "        window_size: Size of the rolling window.\n",
    "        dependent_var: Name of the dependent variable column.\n",
    "        independent_vars: List of names of independent variable columns.\n",
    "        reg_type: Type of regression to perform ('OLS', 'Ridge', 'Lasso'). Default is 'OLS'.\n",
    "        alpha: Regularization strength for Ridge and Lasso. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with the regression coefficients for each window, indexed by the original DataFrame's index.\n",
    "        Returns None if there are issues.\n",
    "    \"\"\"\n",
    "\n",
    "    n_rows = len(df)\n",
    "    results = []\n",
    "\n",
    "    def _regress_window(i):\n",
    "        if i < window_size - 1:\n",
    "            return {'index': df.index[i],\n",
    "                   'intercept': np.nan,\n",
    "                   **dict(zip(independent_vars, [np.nan] * len(independent_vars)))}\n",
    "\n",
    "        window_data = df.iloc[i - window_size + 1:i + 1]\n",
    "\n",
    "        X = window_data[independent_vars].values\n",
    "        y = window_data[dependent_var].values.reshape(-1,1)\n",
    "\n",
    "        # Return NaN coefficients if window contains NaN or is incomplete\n",
    "        if len(window_data) < window_size or np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "            return {'index': df.index[i],\n",
    "                   'intercept': np.nan,\n",
    "                   **dict(zip(independent_vars, [np.nan] * len(independent_vars)))}\n",
    "\n",
    "        if reg_type.upper() == 'OLS':\n",
    "            model = LinearRegression()\n",
    "        elif reg_type.upper() == 'RIDGE':\n",
    "            model = Ridge(alpha=alpha)\n",
    "        elif reg_type.upper() == 'LASSO':\n",
    "            model = Lasso(alpha=alpha)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reg_type. Choose 'OLS', 'Ridge', or 'Lasso'.\")\n",
    "\n",
    "        model.fit(X, y)\n",
    "        coefs = model.coef_.flatten()\n",
    "        intercept = model.intercept_\n",
    "\n",
    "        return {'index': df.index[i], 'intercept': intercept, **dict(zip(independent_vars, coefs))}\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(_regress_window)(i) for i in range(n_rows))\n",
    "\n",
    "    # All results should be valid now since we're returning NaN instead of None\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Handle multiindex\n",
    "    if isinstance(results_df['index'].iloc[0], tuple):\n",
    "        results_df[list(df.index.names)] = results_df['index'].apply(pd.Series)\n",
    "        results_df = results_df.drop(columns='index').set_index(list(df.index.names))\n",
    "    else:\n",
    "        results_df = results_df.set_index('index')\n",
    "\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033c2a9-5a5f-453c-8db9-f4b21374fd76",
   "metadata": {},
   "source": [
    "### calculate simple returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c826f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a947e8-2cc8-442a-89da-24bffc9bb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = calculate_simple_returns(mdf).set_index(['ID','DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5d9b1-81b2-4a39-be75-7fe982a10267",
   "metadata": {},
   "source": [
    "### calculate low volatility factors | analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_period = \"monthly\"\n",
    "\n",
    "if calc_period == \"daily\":\n",
    "    beta_period = 252\n",
    "    vol_period = 63\n",
    "    fwd_ret_period = 63\n",
    "elif calc_period == \"weekly\":\n",
    "    beta_period = 52\n",
    "    vol_period = 26\n",
    "    fwd_ret_period = 13\n",
    "elif calc_period == \"monthly\":\n",
    "    beta_period = 24\n",
    "    vol_period = 24\n",
    "    fwd_ret_period = 3\n",
    "\n",
    "print(beta_period, vol_period, fwd_ret_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81939a38-1777-49ea-a478-5a95ae2f5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mdf['beta'] = mdf\\\n",
    ".groupby('ID', group_keys = False).apply(lambda x: rolling_regression(x, window_size =  beta_period, dependent_var = 'stock_return', independent_vars = ['idx_return']))\\\n",
    ".drop(columns = 'intercept')\\\n",
    ".rename(columns = {'idx_return':'beta'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "mdf['volatility'] = mdf.groupby('ID',group_keys = False)['stock_return'].rolling(vol_period).std().mul(np.sqrt(252)).reset_index(level = 0, drop = True).rename('volatility')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f850f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "mdf['avg_volm_to_cap'] = mdf.groupby('ID', group_keys = False).apply(lambda x: x['volume'].rolling(vol_period).mean()/(x['market_cap']/1000000)).rename('avg_volm_to_cap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f489239",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mdf['volume_trend'] = mdf.groupby('ID', group_keys = False).apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = vol_period , dependent_var = 'volume', independent_vars = ['trend'])).rename(columns = {'trend':'volume_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c246519",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "mdf['residual_variance'] = mdf.groupby('ID', group_keys = False).apply(lambda x: rolling_residual_variance(x, window_size = vol_period , dependent_var = 'stock_return', independent_vars = ['idx_return']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60740c26-8a26-449a-a2ea-3910aebe749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_returns(returns):\n",
    "    return (1 + returns).prod() - 1\n",
    "\n",
    "# List of rolling periods to calculate\n",
    "periods = [1, 2, 3, 6, 12]\n",
    "\n",
    "# Calculate rolling compounded returns for each period\n",
    "for period in periods:\n",
    "    # Stock returns\n",
    "    mdf[f'stock_return_{period}m'] = mdf.groupby('ID')['stock_return'].rolling(\n",
    "        window=period, min_periods=period\n",
    "    ).apply(compound_returns).reset_index(level = 0, drop = True).values\n",
    "\n",
    "    # SPY returns\n",
    "    mdf[f'idx_return_{period}m'] = mdf.groupby('ID')['idx_return'].rolling(\n",
    "        window=period, min_periods=period\n",
    "    ).apply(compound_returns).reset_index(level = 0, drop = True).values\n",
    "\n",
    "    # Calculate excess returns (stock - spy)\n",
    "    mdf[f'rs_{period}m'] = mdf[f'stock_return_{period}m'] - mdf[f'idx_return_{period}m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43d574-05a4-4400-b0c3-325e800698ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mdf['3mrs_3mago'] = mdf.groupby('ID')['rs_3m'].shift(3)\n",
    "mdf['3mrs_6mago'] = mdf.groupby('ID')['rs_3m'].shift(6)\n",
    "mdf['3mrs_9mago'] = mdf.groupby('ID')['rs_3m'].shift(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d3c92-c1e5-4eaa-b948-0fa50d85b630",
   "metadata": {},
   "source": [
    "### value factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984e12f-db95-44b0-8c8a-6ddde4035c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earnings to price"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1607a63-3a55-4f0f-b9f1-c6444b94c219",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mdf['eps_to_price'] = mdf.groupby('ID').apply(lambda x: x['eps']/x['px_last_splits']).reset_index(0, drop = True)\n",
    "\n",
    "\n",
    "mdf['eps_to_price_trend']= mdf.groupby('ID', group_keys = False)\\\n",
    ".apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = 24 , dependent_var = 'eps_to_price', independent_vars = ['trend']))\\\n",
    ".rename(columns = {'trend':'eps_to_price_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0b4cb-45f7-4b74-87b7-04e6d867cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales to price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8480d-892a-4371-be6d-086c4dfb8da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "63501266-5c3c-42c3-afce-6638cdf97e32",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mdf['sales_to_price'] = mdf.groupby('ID').apply(lambda x: x['sales']/x['px_last_splits']).reset_index(0, drop = True)\n",
    "\n",
    "\n",
    "mdf['sales_to_price_trend']= mdf.groupby('ID', group_keys = False)\\\n",
    ".apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = 24 , dependent_var = 'sales_to_price', independent_vars = ['trend']))\\\n",
    ".rename(columns = {'trend':'sales_to_price_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6717c-6556-4bb5-87a7-6156c7d4668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cash to price"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a77f91f-f957-4121-baa7-1364e3913ac1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mdf['fcf_calc'] = mdf['cfo_ltm_a'] + mdf['capex'] + mdf['dvd'] \n",
    "\n",
    "mdf['cash_to_price'] = mdf['fcf_calc'] / mdf['px_last_splits']\n",
    "\n",
    "mdf['cash_to_price_trend']= mdf.groupby('ID', group_keys = False)\\\n",
    ".apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = 24 , dependent_var = 'cash_to_price', independent_vars = ['trend']))\\\n",
    ".rename(columns = {'trend':'cash_to_price_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ffa15-18ad-46be-8ad7-c90220cae30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividend to price "
   ]
  },
  {
   "cell_type": "raw",
   "id": "be2898b4-054b-4d0f-90b6-586d2487860e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mdf['div_to_price'] = np.abs(mdf['dvd']) / mdf['px_last_splits']\n",
    "\n",
    "mdf['div_to_price_trend']= mdf.groupby('ID', group_keys = False)\\\n",
    ".apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = 24 , dependent_var = 'div_to_price', independent_vars = ['trend']))\\\n",
    ".rename(columns = {'trend':'div_to_price_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354adda-5784-4d19-8d50-fba04d4695b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book to price\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15bf8cca-9026-4ee1-9f50-91efcef1d7f7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mdf['book_to_price'] = mdf['book_value'] / mdf['cur_mkt_cap']\n",
    "\n",
    "mdf['book_to_price_trend']= mdf.groupby('ID', group_keys = False)\\\n",
    ".apply(lambda x: rolling_regression(x.assign(trend = lambda x:np.arange(len(x))), window_size = 24 , dependent_var = 'book_to_price', independent_vars = ['trend']))\\\n",
    ".rename(columns = {'trend':'book_to_price_trend'}).drop(columns = 'intercept')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76146826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward_rets(mdf, fwd_ret_period):\n",
    "    mdf['forward_return'] = (mdf.groupby('ID')['close']\n",
    "                          .shift(-fwd_ret_period) / mdf['close'] - 1)\n",
    "\n",
    "    mdf['idx_forward_return'] = (mdf.groupby('ID')['idx_close']\n",
    "                            .shift(-fwd_ret_period) / mdf['idx_close'] - 1)\n",
    "\n",
    "    mdf['relative_return'] = (\n",
    "            mdf['forward_return'] - mdf['idx_forward_return']\n",
    "        )\n",
    "    \n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd366414",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = calc_forward_rets(mdf, fwd_ret_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b4750-cdb5-467e-b490-a51d8edb1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.to_csv('data_analytics_univ_rty_2014_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26392c75-575e-45d9-b871-7a40fb15d996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
